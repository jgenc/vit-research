{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer\n",
    "\n",
    "In this notebook we will implement and use [Brian Pulfer's](https://www.brianpulfer.ch/blog/vit) Vision Transformer, which is based on the original paper's implementation,\n",
    "found [here](https://arxiv.org/abs/2010.11929)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/jgenc/vit-research/blob/main/resources/ViT_Steps.png?raw=1\" alt=\"Patches\" width=\"1000\"/>\n",
    "\n",
    "Image source: https://arxiv.org/abs/2202.06709"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement the model in the following steps:\n",
    "1. Transform input image into patches\n",
    "2. Use Linear Mapping to create tokens from the patches\n",
    "3. Create trainable classification token\n",
    "4. Add positional embedding to each token\n",
    "5. Embedding Layer\n",
    "6. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue we will need to download some Python files including\n",
    "helpful tooling functions, such as plotting, the train loop, etc.\n",
    "If the project is cloned locally, uncomment the commented lines and comment the\n",
    "lines starting with an exclamantion point. The repository is found [here](https://github.com/jgenc/vit-research)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if running on local machine\n",
    "# %pip install venv\n",
    "# !python3 -m venv .venv\n",
    "# %pip install -r requirements.txt\n",
    "\n",
    "!mkdir utils\n",
    "!curl https://raw.githubusercontent.com/jgenc/vit-research/main/utils/patchifying.py > ./utils/patchifying.py\n",
    "!curl https://raw.githubusercontent.com/jgenc/vit-research/main/utils/train_loop.py > ./utils/train_loop.py\n",
    "!curl https://raw.githubusercontent.com/jgenc/vit-research/main/utils/plotting.py > ./utils/plotting.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import VisionTransformer\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from utils.plotting import plot_results, show_metrics, save_metrics, plot_results\n",
    "from utils.train_loop import train_loop\n",
    "from utils.patchifying import show_patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Dataset\n",
    "\n",
    "We will use the **MNIST** dataset, which contains $28 \\times 28$ grayscale images.\n",
    "More info about the dataset can be found [here](https://en.wikipedia.org/wiki/MNIST_database).\n",
    "\n",
    "The class `torchvision.dataset.mnist.MNIST()` contains tuple objects of images\n",
    "and the target label, in the `(img, target)` format. `img` is a [PIL image](https://python-pillow.org/). Currently the images are not in the format PyTorch needs,\n",
    "therefore we use the `torchvision.transforms.ToTensor()` class, which transforms an image to a Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = ToTensor()\n",
    "\n",
    "train_set = MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_set = MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "print(f\"Train set size: {len(train_set)}, Test set size: {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training split of the dataset contains $60.000$ images, while the test\n",
    "set contains $10.000$ images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to create a dataloader for each split of the dataset. It handles\n",
    "the data loading using an iterator (which happens iteratively, meaning the whole dataset is not\n",
    "loaded at one time, only `batch_size` images are loaded into memory).\n",
    "\n",
    "The `shuffle` parameter is also useful, it randomly shuffles the dataset\n",
    "on each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells prints an image from the train dataset.\n",
    "\n",
    "We can change the printed image by changing the number of the index variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0                       # From 0 to 59.999\n",
    "image = train_set[index][0]\n",
    "image = image.transpose(0, 1).transpose(1, 2)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(image, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Creating Patches\n",
    "\n",
    "To create the patches we need to pick a value for the $p$ parameter. This process\n",
    "will create $m$ patches, where $$ m = \\frac{hw}{p^2} $$\n",
    "\n",
    "Our dataset has a size of $(60.000, 1, 28, 28)$. We will use $p=4$, and the number\n",
    "of patches will be $m=49$.\n",
    "\n",
    "The size dimensions of each patch are $p \\times p$, in our case $4 \\times 4$.\n",
    "Each image will be broken down into a $\\sqrt{m} \\times \\sqrt{m}$ grid, in our case\n",
    "the grid will be $7 \\times 7$.\n",
    "\n",
    "After these transfromations, the dataset will have a size of $(60.000, 7, 7, 4, 4)$.\n",
    "We need to flatten the dimensions of the grid and each patch into $(60.000, 49, 16)$.\n",
    "\n",
    "\n",
    "<img src=\"./resources/Patches.png\" alt=\"Patches\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows how each image will be split into their patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_patches(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify(images, n_patches):\n",
    "    n, c, h, w = images.shape\n",
    "    assert h == w, \"Patchify method is implemented for square images only\"\n",
    "\n",
    "    patches = torch.zeros(n, n_patches**2, h * w * c // n_patches**2)\n",
    "    patch_size = h // n_patches\n",
    "    patches = images.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
    "    patches = patches.flatten(2, 3).flatten(3, 4).flatten(1, 2)\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Linear Embedding\n",
    "\n",
    "Now we need to transform the patches into tokens, which is the same process as the\n",
    "traditional [Transformer](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "This step includes the tuning of the `hidden_d` parameter, which is a Hyper Parameter\n",
    "of the model. In our simple training examples we will use a value of $8$.\n",
    "There's also the `input_d` parameter, which is calculated with $p^2$. We chose\n",
    "a value of $4$ for $p$, so `input_d` will have a value of $16$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyViT(torch.nn.Module):\n",
    "    def __init__(self, chw=(1, 28, 28), n_patches=7):\n",
    "        super(MyViT, self).__init__()\n",
    "\n",
    "        self.chw = chw  # (C, H, W)\n",
    "        self.n_patches = n_patches\n",
    "\n",
    "        assert (\n",
    "            chw[1] % n_patches == 0\n",
    "        ), \"Input shape not entirely divisible by number of patches\"\n",
    "\n",
    "        assert (\n",
    "            chw[2] % n_patches == 0\n",
    "        ), \"Input shape not entirely divisible by number of patches\"\n",
    "\n",
    "        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
    "\n",
    "        # 1) Linear mapper\n",
    "        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n",
    "        self.linear_mapper = torch.nn.Linear(self.input_d, self.hidden_d)\n",
    "\n",
    "    def forward(self, images):\n",
    "        patches = patchify(images, self.n_patches)\n",
    "        tokens = self.linear_mapper(patches)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Classification token \\<cls\\>\n",
    "\n",
    "It's a learnable parameter of the model. We will need to add it to the tokens\n",
    "we created in the previous step.\n",
    "\n",
    "This token's size is `(1, hidden_d)`. If we add this to the tokens we have already\n",
    "made, we will have a size of $(60.000, 50, 16)$ for the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We update the `MyVit` class with the \\<cls\\> token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyViT(torch.nn.Module):\n",
    "    def __init__(self, chw=(1, 28, 28), n_patches=7):\n",
    "        super(MyViT, self).__init__()\n",
    "\n",
    "        self.chw = chw  # (C, H, W)\n",
    "        self.n_patches = n_patches\n",
    "\n",
    "        assert (\n",
    "            chw[1] % n_patches == 0\n",
    "        ), \"Input shape not entirely divisible by number of patches\"\n",
    "\n",
    "        assert (\n",
    "            chw[2] % n_patches == 0\n",
    "        ), \"Input shape not entirely divisible by number of patches\"\n",
    "\n",
    "        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
    "        # 1) Linear mapper\n",
    "        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n",
    "        self.linear_mapper = torch.nn.Linear(self.input_d, self.hidden_d)\n",
    "\n",
    "        # 2) Learnable classifiation token\n",
    "        self.class_token = torch.nn.Parameter(torch.rand(1, self.hidden_d))\n",
    "\n",
    "    def forward(self, images):\n",
    "        patches = patchify(images, self.n_patches)\n",
    "        tokens = self.linear_mapper(patches)\n",
    "\n",
    "        # Adding classification token to the tokens\n",
    "        tokens = torch.stack(\n",
    "            [torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))]\n",
    "        )\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Positional Encoding\n",
    "\n",
    "The model up until now does not save the positional information\n",
    "of each patch in the original image. However, that information is crucial for any\n",
    "image task, as we would not like to return an image with jumbled patches.\n",
    "\n",
    "To solve this we will create the positional embeddings of each token using the\n",
    "sin and cos functions.\n",
    "\n",
    "The following function needs the `sequence_length` and `hidden_d` parameters.\n",
    "`sequence_length` is the number of tokens created for each image (which is $50$)\n",
    "and `hidden_d` is a hyperparameter we defined earlier (which is $8$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_embeddings(sequence_length, hidden_d):\n",
    "    result = torch.ones(sequence_length, hidden_d)\n",
    "\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(hidden_d):\n",
    "            result[i][j] = (\n",
    "                np.sin(i / (10000 ** (j / hidden_d)))\n",
    "                if j % 2 == 0\n",
    "                else np.cos(i / (10000 ** ((j - 1) / hidden_d)))\n",
    "            )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell plots the positional embeddings using the values of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(get_positional_embeddings(49 + 1, 8), cmap=\"hot\", interpolation=\"nearest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can increase the size of the parameters to see how the positional encodings\n",
    "would look for bigger models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(get_positional_embeddings(100, 300), cmap=\"hot\", interpolation=\"nearest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We update the `MyViT` class with the new functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyViT(torch.nn.Module):\n",
    "    def __init__(self, chw=(1, 28, 28), n_patches=7):\n",
    "        super(MyViT, self).__init__()\n",
    "\n",
    "        self.chw = chw  # (C, H, W)\n",
    "        self.n_patches = n_patches\n",
    "\n",
    "        assert (\n",
    "            chw[1] % n_patches == 0\n",
    "        ), \"Input shape not entirely divisible by number of patches\"\n",
    "\n",
    "        assert (\n",
    "            chw[2] % n_patches == 0\n",
    "        ), \"Input shape not entirely divisible by number of patches\"\n",
    "\n",
    "        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
    "\n",
    "        # 1) Linear mapper\n",
    "        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n",
    "        self.linear_mapper = torch.nn.Linear(self.input_d, self.hidden_d)\n",
    "\n",
    "        # 2) Learnable classifiation token\n",
    "        self.class_token = torch.nn.Parameter(torch.rand(1, self.hidden_d))\n",
    "\n",
    "        # 3) Positional embedding\n",
    "        self.pos_embed = torch.nn.Parameter(\n",
    "            torch.tensor(\n",
    "                get_positional_embeddings(self.n_patches**2 + 1, self.hidden_d)\n",
    "            )\n",
    "        )\n",
    "        self.pos_embed.requires_grad = False\n",
    "\n",
    "    def forward(self, images):\n",
    "        patches = patchify(images, self.n_patches)\n",
    "        tokens = self.linear_mapper(patches)\n",
    "\n",
    "        # Adding classification token to the tokens\n",
    "        tokens = torch.stack(\n",
    "            [torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))]\n",
    "        )\n",
    "\n",
    "        # Adding positional embedding\n",
    "        pos_embed = self.pos_embed.repeat(n, 1, 1)\n",
    "\n",
    "        out = tokens + pos_embed\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Encoder Module\n",
    "\n",
    "In this step we will impelement the following:\n",
    "\n",
    "- Multi-Head Self-Attention (MSA)\n",
    "- Multi-Layer Perceptron (MLP)\n",
    "\n",
    "Before we apply each of these blocks we will create residual connetions and\n",
    "pass the input through a Layer Normalisation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMSA(torch.nn.Module):\n",
    "    def __init__(self, d, n_heads=2):\n",
    "        super(MyMSA, self).__init__()\n",
    "\n",
    "        self.d = d\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n",
    "\n",
    "        d_head = int(d / n_heads)\n",
    "\n",
    "        self.q_mappings = torch.nn.ModuleList(\n",
    "            [torch.nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
    "        )\n",
    "        self.k_mappings = torch.nn.ModuleList(\n",
    "            [torch.nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
    "        )\n",
    "        self.v_mappings = torch.nn.ModuleList(\n",
    "            [torch.nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
    "        )\n",
    "\n",
    "        self.d_head = d_head\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        # Sequences has shape (N, seq_length, token_dim)\n",
    "        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n",
    "        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n",
    "\n",
    "        result = []\n",
    "\n",
    "        for sequence in sequences:\n",
    "            seq_result = []\n",
    "\n",
    "            for head in range(self.n_heads):\n",
    "                q_mapping = self.q_mappings[head]\n",
    "                k_mapping = self.k_mappings[head]\n",
    "                v_mapping = self.v_mappings[head]\n",
    "\n",
    "                seq = sequence[:, head * self.d_head : (head + 1) * self.d_head]\n",
    "\n",
    "                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
    "\n",
    "                attention = self.softmax(q @ k.T / (self.d_head**0.5))\n",
    "\n",
    "                seq_result.append(attention @ v)\n",
    "\n",
    "            result.append(torch.hstack(seq_result))\n",
    "\n",
    "        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyViTBlock(torch.nn.Module):\n",
    "    def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n",
    "        super(MyViTBlock, self).__init__()\n",
    "\n",
    "        self.hidden_d = hidden_d\n",
    "        self.n_heads = n_heads\n",
    "        self.norm1 = torch.nn.LayerNorm(hidden_d)\n",
    "\n",
    "        self.mhsa = MyMSA(hidden_d, n_heads)\n",
    "        self.norm2 = torch.nn.LayerNorm(hidden_d)\n",
    "\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_d, mlp_ratio * hidden_d),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(mlp_ratio * hidden_d, hidden_d),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.mhsa(self.norm1(x))\n",
    "        out = out + self.mlp(self.norm2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We update the `MyVit` class with the Encoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyViT(torch.nn.Module):\n",
    "    def __init__(self, chw, n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10):\n",
    "        super(MyViT, self).__init__()\n",
    "\n",
    "        # Attributes\n",
    "        self.chw = chw  # ( C , H , W )\n",
    "        self.n_patches = n_patches\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_heads = n_heads\n",
    "        self.hidden_d = hidden_d\n",
    "\n",
    "        # Input and patches sizes\n",
    "        assert (\n",
    "            chw[1] % n_patches == 0\n",
    "        ), \"Input shape not entirely divisible by number of patches\"\n",
    "\n",
    "        assert (\n",
    "            chw[2] % n_patches == 0\n",
    "        ), \"Input shape not entirely divisible by number of patches\"\n",
    "\n",
    "        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
    "\n",
    "        # 1) Linear mapper\n",
    "        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n",
    "        self.linear_mapper = torch.nn.Linear(self.input_d, self.hidden_d)\n",
    "\n",
    "        # 2) Learnable classification token\n",
    "        self.class_token = torch.nn.Parameter(torch.rand(1, self.hidden_d))\n",
    "\n",
    "        # 3) Positional embedding\n",
    "        self.register_buffer(\n",
    "            \"positional_embeddings\",\n",
    "            get_positional_embeddings(n_patches**2 + 1, hidden_d),\n",
    "            persistent=False,\n",
    "        )\n",
    "\n",
    "        # 4) Transformer encoder blocks\n",
    "        self.blocks = torch.nn.ModuleList(\n",
    "            [MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)]\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        # Dividing images into patches\n",
    "\n",
    "        n, c, h, w = images.shape\n",
    "\n",
    "        patches = patchify(images, self.n_patches).to(self.positional_embeddings.device)\n",
    "\n",
    "        # Running linear layer tokenization\n",
    "        # Map the vector corresponding to each patch to the hidden size dimension\n",
    "        tokens = self.linear_mapper(patches)\n",
    "\n",
    "        # Adding classification token to the tokens\n",
    "        tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)\n",
    "\n",
    "        # Adding positional embedding\n",
    "        out = tokens + self.positional_embeddings.repeat(n, 1, 1)\n",
    "\n",
    "        # Transformer Blocks\n",
    "        for block in self.blocks:\n",
    "            out = block(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Classification\n",
    "\n",
    "The task of this notebook is to correctly classify the hand-drawn numbers of\n",
    "the MNIST dataset.\n",
    "\n",
    "We have 10 possible classes, meaning the output of the model will need to output\n",
    "10 propabilities. We achieve this by adding an MLP block at the end of the model,\n",
    "with an output dimension of 10 and an input dimension of the previous layer.\n",
    "\n",
    "With a different dataset, which contains a different number of classes, we will\n",
    "need to update the output dimension accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyViT(torch.nn.Module):\n",
    "    def __init__(self, chw, n_patches=7, num_layers=2, hidden_dim=8, num_heads=2, num_classes=10):\n",
    "        super(MyViT, self).__init__()\n",
    "\n",
    "        # Attributes\n",
    "        self.chw = chw  # ( C , H , W )\n",
    "        self.n_patches = n_patches\n",
    "        self.n_blocks = num_layers\n",
    "        self.n_heads = num_heads\n",
    "        self.hidden_d = hidden_dim\n",
    "\n",
    "        # Input and patches sizes\n",
    "        assert (\n",
    "            chw[1] % n_patches == 0\n",
    "        ), \"Input shape not entirely divisible by number of patches\"\n",
    "\n",
    "        assert (\n",
    "            chw[2] % n_patches == 0\n",
    "        ), \"Input shape not entirely divisible by number of patches\"\n",
    "\n",
    "        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
    "\n",
    "        # 1) Linear mapper\n",
    "        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n",
    "        self.linear_mapper = torch.nn.Linear(self.input_d, self.hidden_d)\n",
    "\n",
    "        # 2) Learnable classification token\n",
    "        self.class_token = torch.nn.Parameter(torch.rand(1, self.hidden_d))\n",
    "\n",
    "        # 3) Positional embedding\n",
    "        self.register_buffer(\n",
    "            \"positional_embeddings\",\n",
    "            get_positional_embeddings(n_patches**2 + 1, hidden_dim),\n",
    "            persistent=False,\n",
    "        )\n",
    "\n",
    "        # 4) Transformer encoder blocks\n",
    "        self.blocks = torch.nn.ModuleList(\n",
    "            [MyViTBlock(hidden_dim, num_heads) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        # 5) Classification MLPk\n",
    "        self.mlp = torch.nn.Sequential(torch.nn.Linear(self.hidden_d, num_classes), torch.nn.Softmax(dim=-1))\n",
    "\n",
    "    def forward(self, images):\n",
    "        # Dividing images into patches\n",
    "        n, c, h, w = images.shape\n",
    "        patches = patchify(images, self.n_patches).to(self.positional_embeddings.device)\n",
    "\n",
    "        # Running linear layer tokenization\n",
    "        # Map the vector corresponding to each patch to the hidden size dimension\n",
    "        tokens = self.linear_mapper(patches)\n",
    "\n",
    "        # Adding classification token to the tokens\n",
    "        tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)\n",
    "\n",
    "        # Adding positional embedding\n",
    "        out = tokens + self.positional_embeddings.repeat(n, 1, 1)\n",
    "\n",
    "        # Transformer Blocks\n",
    "        for block in self.blocks:\n",
    "            out = block(out)\n",
    "\n",
    "        # Getting the classification token only\n",
    "        out = out[:, 0]\n",
    "\n",
    "        return self.mlp(out)  # Map to output dimension, output category distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Vision Transformer on MNIST\n",
    "\n",
    "Your environment might have a GPU, TPU or the MPS chip made by Apple.\n",
    "If the accelerator device actually exists it would be better to train the model\n",
    "on that device rather than on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an instance of the model with values of the parameters we used in the\n",
    "previous cells.\n",
    "\n",
    "We can experiment with different values for each parameter. Notably:\n",
    "- `num_layers` indicates the number of encoder blocks containing the MSA and MLP blocks\n",
    "- `hidden_d` is used to change the dimensionality of the hidden layers\n",
    "- `num_heads` is the parameter which indicates how many parallel multiplications\n",
    "will be computed on the Q, K, V matrices. In the Figure 2 at the start of the\n",
    "model you can look at the $(c)$ part\n",
    "- `num_classes` is the number that handles the output dimension size. As we said\n",
    "before, this number depends on the task and the dataset used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 2\n",
    "hidden_dim = 8\n",
    "num_heads = 2\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyViT(\n",
    "    chw=(1, 28, 28),\n",
    "    n_patches=7,\n",
    "    num_layers=num_layers,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_heads=num_heads,\n",
    "    num_classes=num_classes, \n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to pick a number of epochs and the Learing rate, just like every\n",
    "Deep Learning project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 5\n",
    "LR = 5e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_metric = []\n",
    "train_accuracy_metric = []\n",
    "test_loss_metric = []\n",
    "test_accuracy_metric = []\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in trange(N_EPOCHS, desc=\"Training\"):\n",
    "    correct, total = 0, 0\n",
    "    train_loss = 0.0\n",
    "    # Train\n",
    "    for batch in tqdm(\n",
    "        train_loader, desc=f\"Epoch {epoch + 1} in training\", leave=False, miniters=1\n",
    "    ):\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_hat = model(x)\n",
    "        loss = criterion(y_hat, y)\n",
    "\n",
    "        train_loss += loss.detach().cpu().item() / len(train_loader)\n",
    "        # print(f\"\\n\\nTrain Loss: {train_loss}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n",
    "        total += len(x)\n",
    "        # print(f\"[TRAIN] We have {correct} corrects and {total} total\")\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}/{N_EPOCHS} \\nTrain loss: {train_loss:.2f}, Train accuracy: {correct / total * 100:.2f}%\"\n",
    "    )\n",
    "\n",
    "    train_loss_metric.append(train_loss)\n",
    "    train_accuracy_metric.append(correct / total * 100)\n",
    "\n",
    "    # Test\n",
    "    with torch.no_grad():\n",
    "        correct_test, total_test = 0, 0\n",
    "        test_loss = 0.0\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "\n",
    "            test_loss += loss.detach().cpu().item() / len(test_loader)\n",
    "            correct_test += (\n",
    "                torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n",
    "            )\n",
    "            total_test += len(x)\n",
    "        print(f\"Test loss: {test_loss:.2f}\")\n",
    "        print(f\"Test accuracy: {correct_test / total_test * 100:.2f}%\")\n",
    "        # print(f\"[TEST] We have {correct_test} corrects and {total_test} total\")\n",
    "    test_loss_metric.append(test_loss)\n",
    "    test_accuracy_metric.append(correct_test / total_test * 100)\n",
    "\n",
    "save_metrics(\n",
    "    num_layers,\n",
    "    num_heads,\n",
    "    hidden_dim,\n",
    "    30,\n",
    "    N_EPOCHS,\n",
    "    LR,\n",
    "    train_accuracy_metric,\n",
    "    test_accuracy_metric,\n",
    "    train_loss_metric,\n",
    "    test_loss_metric\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to look at the performance of the model as the epochs go on, so that we can\n",
    "see if the model is actually learning. We will plot the Accuracy and Loss curves.\n",
    "\n",
    "For the following cell to work the previous cell has to have finished, which can\n",
    "take a while depending on what device the model is trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each training run will save the output in the `metrics` local folder. If you\n",
    "want to plot specific run results than uncomment the `show_metrics` line and\n",
    "change the parameter to the run you want to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(\n",
    "    num_layers,\n",
    "    num_heads,\n",
    "    hidden_dim,\n",
    "    N_EPOCHS,\n",
    "    LR,\n",
    "    test_accuracy_metric,\n",
    "    train_accuracy_metric,\n",
    "    test_loss_metric,\n",
    "    train_loss_metric,\n",
    ")\n",
    "\n",
    "# show_metrics(\"./metrics/23-12-07_21-15-28_metrics.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the model we trained performs better on the test set rather than\n",
    "on the train set. This is not normal behaviour. Our implementation of the Vision\n",
    "Transformer does not implement standard techniques of machine learning, such as\n",
    "Dropout.\n",
    "\n",
    "In the following cells we will try out PyTorch's implementation of ViT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformerGrayscale(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Official PyTorch implementation of Vision Transformer with support for Grayscale (1-channel) images.\n",
    "    Default parameters are for the MNIST dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size=28,\n",
    "        patch_size=7,\n",
    "        num_layers=6,\n",
    "        num_heads=6,\n",
    "        hidden_dim=768,\n",
    "        mlp_dim=3072,\n",
    "        num_classes=10,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super(VisionTransformerGrayscale, self).__init__()\n",
    "        # 1x1 conv to transform 1 channel to 3 channels\n",
    "        self.conv1 = torch.nn.Conv2d(1, 3, 1)\n",
    "        self.vit = VisionTransformer(\n",
    "            image_size=image_size,\n",
    "            patch_size=patch_size,\n",
    "            num_layers=num_layers,\n",
    "            num_heads=num_heads,\n",
    "            hidden_dim=hidden_dim,\n",
    "            mlp_dim=mlp_dim,\n",
    "            num_classes=num_classes,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.vit(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we run the experiments we notice:\n",
    "- The `hidden_dim` parameter has a much bigger value that the one we had previously\n",
    "- The `mpl_dim` parameter is something we did not tinker with on the other implementation\n",
    "- `dropout` is the parameter that handles the rate of the Dropout technique, which was not implemented earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change and try out different values for the following parameters, or we\n",
    "can run the model with the parameters as is. \n",
    "\n",
    "There's also the option of training\n",
    "the model with PyTorch's default parameters. It can be done by uncommenting the\n",
    "line with no given parameters. Note that it will take much more time to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 2\n",
    "hidden_dim = 8\n",
    "num_heads = 2\n",
    "num_classes = 10\n",
    "\n",
    "mlp_dim = 30\n",
    "dropout = 0.1\n",
    "\n",
    "N_EPOCHS = 5\n",
    "LR = 5e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionTransformerGrayscale(\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    hidden_dim=hidden_dim,\n",
    "    mlp_dim=mlp_dim,\n",
    "    num_classes=num_classes,\n",
    "    dropout=dropout,\n",
    ").to(device)\n",
    "\n",
    "# Default Parameters\n",
    "# model = VisionTransformerGrayscale().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_metric = []\n",
    "train_accuracy_metric = []\n",
    "test_loss_metric = []\n",
    "test_accuracy_metric = []\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in trange(N_EPOCHS, desc=\"Training\"):\n",
    "    correct, total = 0, 0\n",
    "    train_loss = 0.0\n",
    "    # Train\n",
    "    for batch in tqdm(\n",
    "        train_loader, desc=f\"Epoch {epoch + 1} in training\", leave=False, miniters=1\n",
    "    ):\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_hat = model(x)\n",
    "        loss = criterion(y_hat, y)\n",
    "\n",
    "        train_loss += loss.detach().cpu().item() / len(train_loader)\n",
    "        # print(f\"\\n\\nTrain Loss: {train_loss}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n",
    "        total += len(x)\n",
    "        # print(f\"[TRAIN] We have {correct} corrects and {total} total\")\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}/{N_EPOCHS} \\nTrain loss: {train_loss:.2f}, Train accuracy: {correct / total * 100:.2f}%\"\n",
    "    )\n",
    "\n",
    "    train_loss_metric.append(train_loss)\n",
    "    train_accuracy_metric.append(correct / total * 100)\n",
    "\n",
    "    # Test\n",
    "    with torch.no_grad():\n",
    "        correct_test, total_test = 0, 0\n",
    "        test_loss = 0.0\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "\n",
    "            test_loss += loss.detach().cpu().item() / len(test_loader)\n",
    "            correct_test += (\n",
    "                torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n",
    "            )\n",
    "            total_test += len(x)\n",
    "        print(f\"Test loss: {test_loss:.2f}\")\n",
    "        print(f\"Test accuracy: {correct_test / total_test * 100:.2f}%\")\n",
    "        # print(f\"[TEST] We have {correct_test} corrects and {total_test} total\")\n",
    "    test_loss_metric.append(test_loss)\n",
    "    test_accuracy_metric.append(correct_test / total_test * 100)\n",
    "\n",
    "save_metrics(\n",
    "    num_layers,\n",
    "    num_heads,\n",
    "    hidden_dim,\n",
    "    mlp_dim,\n",
    "    N_EPOCHS,\n",
    "    LR,\n",
    "    train_accuracy_metric,\n",
    "    test_accuracy_metric,\n",
    "    train_loss_metric,\n",
    "    test_loss_metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(\n",
    "    num_layers,\n",
    "    num_heads,\n",
    "    hidden_dim,\n",
    "    N_EPOCHS,\n",
    "    LR,\n",
    "    test_accuracy_metric,\n",
    "    train_accuracy_metric,\n",
    "    test_loss_metric,\n",
    "    train_loss_metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model's accuracy and loss is not very stable. Also, the\n",
    "training time was significantly less than our previous implementation (this allows\n",
    "us to try and train the model with bigger hyperparameters)\n",
    "\n",
    "We will train the model for more epochs and we will modify model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters from the Paper\n",
    "num_layers = 6\n",
    "hidden_dim = 30\n",
    "num_heads = 6\n",
    "mlp_dim = 40\n",
    "dropout = 0.1\n",
    "num_classes = 10\n",
    "\n",
    "N_EPOCHS = 20\n",
    "LR = 5e-3\n",
    "\n",
    "model = VisionTransformerGrayscale(\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    hidden_dim=hidden_dim,\n",
    "    mlp_dim=mlp_dim,\n",
    "    num_classes=num_classes,\n",
    "    dropout=dropout,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = train_loop(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    device,\n",
    "    num_layers,\n",
    "    num_heads,\n",
    "    hidden_dim,\n",
    "    mlp_dim,\n",
    "    N_EPOCHS,\n",
    "    LR,\n",
    ")\n",
    "\n",
    "train_loss_metric, train_accuracy_metric, test_loss_metric, test_accuracy_metric = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(\n",
    "    num_layers,\n",
    "    num_heads,\n",
    "    hidden_dim,\n",
    "    N_EPOCHS,\n",
    "    LR,\n",
    "    test_accuracy_metric,\n",
    "    train_accuracy_metric,\n",
    "    test_loss_metric,\n",
    "    train_loss_metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is now much more stable and approaches the 100% accuracy rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try and see what the model predicts from a random image in the train dataset.\n",
    "You can try and change the index to another image. Remember, there are 10.000 images\n",
    "in that set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 42                                                                                                                                                                      \n",
    "image, label = test_loader.dataset[index]\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
    "\n",
    "prediction = model(image.unsqueeze(0).to(device))\n",
    "print(f\"Model predicted class {torch.argmax(prediction).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of external applications (such as GIMP) we can draw our own images.\n",
    "I've prepared one such image, which you can see in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "our_image = Image.open(\"./test.png\").convert(\"L\")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(our_image, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a 6. Now we can pass this image to our trained model and see if it will correctly\n",
    "classify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pred = model(transform(our_image).unsqueeze(0).to(device))\n",
    "\n",
    "print(f\"Model predicted class {torch.argmax(pred).item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
